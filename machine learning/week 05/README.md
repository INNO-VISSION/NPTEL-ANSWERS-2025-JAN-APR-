Hereâ€™s the updated README format with questions, options, and answers:  

---

# Machine Learning Quiz Solutions  

### 1.  
Consider a feedforward neural network that performs regression on a ð‘-dimensional input to produce a scalar output. It has ð‘š hidden layers, and each of these layers has ð‘˜ hidden units. What is the total number of trainable parameters in the network? Ignore the bias terms.

a. \( pk + mk^2 + k \)  
b. \( p^2k + mk^2 + k \)  
c. \( pk + mk + k^2 \)  
d. \( p^2k^2 + mk + k \)  

**Answer:** a. \( pk + mk^2 + k \)  

### 2. What is the weight update rule in a Perceptron during training?  

a. \( W_{ij} = W_{ij} + x_j \)  
b. \( I(\sum_{k=1}^{p} W_{ik} x_k > 0) x_j \)  
c. \( W_{ij} = W_{ij} + \eta x_j \)  
d. \( W_{ij} = W_{ij} - \eta x_j \)  

**Answer:** b. \( I(\sum_{k=1}^{p} W_{ik} x_k > 0) x_j \)  

### 3.  Why does backpropagation in deep networks suffer from the vanishing gradient problem?  
 
a. \( \nabla_{W(A)}(y) \) depends on \( W(A) \) only  
b. \( \nabla_{W(A)}(y) \) depends on \( W(B) \)  
c. Gradients get larger in deeper layers  
d. Weight updates are not required in deep networks  

**Answer:** b. \( \nabla_{W(A)}(y) \) depends on \( W(B) \)  

### 4.  Which of the following are true for the activation function ReLU?  

a. It is non-linear  
b. It has a vanishing gradient problem  
c. It is computationally efficient  
d. It allows sparse activation  

**Answer:** a, c, d  

### 5.  
**Question:** Which of the following are advantages of batch normalization?  

**Options:**  
a. Reduces internal covariate shift  
b. Allows higher learning rates  
c. Increases computation cost significantly  
d. Regularizes the network  

**Answer:** a, b, d  

### 6.  What is the entropy of a probability distribution where the probability of three outcomes is \( p = [0.5, 0.25, 0.25] \)?  

a. 0.5  
b. 1.0  
c. 0.333  
d. 2.0  

**Answer:** c. 0.333  

### 7.  Which of the following are true about dropout regularization?  

a. It helps prevent overfitting  
b. It increases the number of parameters  
c. It randomly drops units during training  
d. It acts as an ensemble method  

**Answer:** a, c, d  

### 8.  Which of the following are loss functions used for classification?  

a. Cross-entropy loss  
b. Hinge loss  
c. Mean squared error  
d. L1 loss  

**Answer:** a, b  

### 9. What is the measure used in Kullback-Leibler divergence?  

 
a. \( D_{KL}(q || p) \)  
b. \( D_{KL}(p || q) = D_{KL}(q || p) \)  
c. \( D_{KL}(p || q) \)  
d. \( D_{KL}(p || q) + D_{KL}(q || p) \)  

**Answer:** c. \( D_{KL}(p || q) \)  

### 10. Which of the following statements about activation functions are incorrect?  

a. Sigmoid has an output range of \([-1,1]\)  
b. Tanh has an output range of \([0,1]\)  
c. ReLU is linear for negative inputs  
d. Softmax is typically used for multi-class classification  

**Answer:** a, b, c  
